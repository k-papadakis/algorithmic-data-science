{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithmic Data Science - Exercise Series 1\n",
    "\n",
    "**Konstantinos Papadakis**\n",
    "\n",
    "*Data Science and Machine Learning 03400149*\n",
    "\n",
    "*k.i.papadakis@gmail.com*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "<img src=\"images/exercise1.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/6-3-1.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singleton Counts\n",
      "1: 4\n",
      "2: 6\n",
      "3: 8\n",
      "4: 8\n",
      "5: 6\n",
      "6: 4\n",
      "\n",
      "Pair Counts\n",
      "[[4 2 3 2 1 0]\n",
      " [2 6 3 4 2 1]\n",
      " [3 3 8 4 4 2]\n",
      " [2 4 4 8 3 3]\n",
      " [1 2 4 3 6 2]\n",
      " [0 1 2 3 2 4]]\n",
      "\n",
      "Hashes\n",
      "(1, 2): 2\n",
      "(1, 3): 3\n",
      "(1, 4): 4\n",
      "(1, 5): 5\n",
      "(1, 6): 6\n",
      "(2, 3): 6\n",
      "(2, 4): 8\n",
      "(2, 5): 10\n",
      "(2, 6): 1\n",
      "(3, 4): 1\n",
      "(3, 5): 4\n",
      "(3, 6): 7\n",
      "(4, 5): 9\n",
      "(4, 6): 2\n",
      "(5, 6): 8\n",
      "\n",
      "Frequent Buckets\n",
      "1 2 4 8\n",
      "\n",
      "PCY Second Pass Pairs\n",
      "(1, 2) (1, 4) (2, 4) (2, 6) (3, 4) (3, 5) (4, 6) (5, 6)\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import collections\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "baskets = np.array([\n",
    "    [1, 2, 3], [2, 3, 4], [3, 4, 5], [4, 5, 6],\n",
    "    [1, 3, 5], [2, 4, 6], [1, 3, 4], [2, 4, 5],\n",
    "    [3, 5, 6], [1, 2, 4], [2, 3, 5], [3, 4, 6],\n",
    "], dtype=int)\n",
    "threshold = 4\n",
    "n_buckets = 11\n",
    "\n",
    "def hash_(i, j):\n",
    "    return (i * j) % n_buckets\n",
    "\n",
    "\n",
    "# (a) Compute the supports \n",
    "item_names, item_freqs = np.unique(baskets, return_counts=True)\n",
    "\n",
    "pair_freqs = np.zeros((6, 6), dtype=int)\n",
    "for basket in baskets:\n",
    "    for x in basket:\n",
    "        for y in basket:\n",
    "            pair_freqs[x-1][y-1] += 1  # 0 based indexing, should be a triangular matrix normally\n",
    "\n",
    "print('Singleton Counts')\n",
    "for name, count in zip(item_names, item_freqs):\n",
    "    print(f'{name}: {count}')\n",
    "print()\n",
    "print('Pair Counts')\n",
    "print(pair_freqs)\n",
    "print()\n",
    "\n",
    "# (b) Hashes\n",
    "print('Hashes')\n",
    "for pair in itertools.combinations(item_names, 2):\n",
    "    print(f'{pair}: {hash_(*pair)}')\n",
    "print()\n",
    "\n",
    "# (c) Frequent bucket\n",
    "bucket_freqs = np.zeros(n_buckets, dtype=int)\n",
    "for basket in baskets:\n",
    "    for pair in itertools.combinations(basket, 2):\n",
    "        bucket_freqs[hash_(*pair)] += 1\n",
    "\n",
    "print('Frequent Buckets')\n",
    "print(*(bucket for bucket, freq in enumerate(bucket_freqs) if freq >= threshold))\n",
    "print()\n",
    "\n",
    "# (c) PCY second pass pairs\n",
    "pcy_freq_pairs = []\n",
    "for pair in itertools.combinations(item_names, 2):\n",
    "    if bucket_freqs[hash_(*pair)] >= threshold:\n",
    "        pcy_freq_pairs.append(pair)\n",
    "print('PCY Second Pass Pairs')\n",
    "print(*(pcy_freq_pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/6-3-2.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second Pass Bucket Frequencies\n",
      "[0 5 5 0 6 0 0 0 6]\n",
      "\n",
      "Frequent Pairs after the Second Pass\n",
      "(1, 2) (1, 4) (2, 4) (2, 6) (3, 4) (3, 5) (4, 6) (5, 6)\n"
     ]
    }
   ],
   "source": [
    "def hash_2(i, j):\n",
    "    return (i * j) % n_buckets\n",
    "\n",
    "n_buckets_2 = 9\n",
    "bucket_freqs_2 = np.zeros(n_buckets_2, int)\n",
    "first_pass_candidates = set(pcy_freq_pairs)  # should be a bitmap normally\n",
    "\n",
    "# Stage 2\n",
    "for basket in baskets:\n",
    "    for pair in itertools.combinations(basket, 2):\n",
    "        if pair in first_pass_candidates:\n",
    "            bucket_freqs_2[hash_2(*pair)] += 1\n",
    "\n",
    "stage_2_pairs = []\n",
    "for pair in pcy_freq_pairs:\n",
    "    if bucket_freqs_2[hash_2(*pair)] >= threshold:\n",
    "        stage_2_pairs.append(pair)\n",
    "        \n",
    "print('Second Pass Bucket Frequencies')\n",
    "print(bucket_freqs_2)\n",
    "print()\n",
    "print('Frequent Pairs after the Second Pass')\n",
    "print(*stage_2_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second hashing didn't reduce the frequent pairs, which is quite unfortunate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/6-4-1.png\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, since the maximal frequent itemsets all have cardinality $\\leq 2$, any itemset of cardinality $>3$ can't be in the border, since we can remove one element and the result will still be infrequent.\n",
    "\n",
    "Also, the frequent singletons are $\\{A\\}, \\{B\\}, \\{C\\}, \\{D\\}, \\{E\\}$, and together with $\\{A, B\\}, \\{B, C\\}, \\{A, C\\}, \\{A, D\\}$ they make all the frequent itemsets.\n",
    "\n",
    "The singletons belong in the border, that are not equal to the above singletons are in the border.\n",
    "\n",
    "$E$ along with any of the $A, B, C, D$ is in the border.\n",
    "\n",
    "And finally, from the union of pairs from $\\{A, B\\}, \\{B, C\\}, \\{A, C\\}, \\{A, D\\}$, only $\\{A, B, C\\}$ is in the border.\n",
    "\n",
    "**To summarize,** the border is:\n",
    "$\\{F\\}, \\{G\\}, \\{H\\}, \\{A, E\\}, \\{B, E\\}, \\{C, E\\}, \\{D, E\\}, \\{A, B, C\\}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "<img src=\"images/exercise2.png\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Correctness of the A-Priori Algorithm\n",
    "\n",
    "In the A-Priori algorithm, \n",
    "\n",
    "* At first we count the frequencies of all singletons and drop any that are not frequent.\n",
    "\n",
    "* Using the result of the filtering, we construct all pairs, count their frequencies again, and drop the pairs that are not frequent.\n",
    "\n",
    "* Continuing in this manner, given the frequent itemsets of cardinality k, we construct itemsets of cardinality k+1.\n",
    "\n",
    "The algorithm produces correct results because all subsets of a frequent itemset are themselves frequent, meaning that we can construct any frequent itemset from its frequent subsets (the frequent itemsets of one size smaller are enough to achieve this).\n",
    "\n",
    "The A-Priori algorithm requires one pass everytime we increase the itemset cardinality, so that it can count the frequencies.\n",
    "\n",
    "### (b) Complexity of the A-Priori Algorithm\n",
    "\n",
    "Consider the extreme case where the threshold is set to 0, meaning that all itemsets are considered frequent. In that case, we are building all subsets of cardinality $K$ of a set of cardinality $N$, therefore the complexity is of the order $\\binom{N}{K}$ which is not polynomial neither in $N$ nor in $K$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) (i) Correctness of the FP-Growth Algorithm\n",
    "\n",
    "The FP-Growth algorithm attempts to efficiently store the dataset in a Trie data structure. The items of each basket are inserted in the trie in decreasing order of their frequency, so that the chance of creating redundant paths in the Trie becomes smaller. Each node in the Trie also contains a pointer to the next node that contains the same key/item (\"next\" by order of insertion). Each node contains a value the number of times a path crossed it during insertion. A pointer to the first node for each item is stored, so that all Trie nodes containing that item value can be traversed as a Linked List. In a adition to that, each node contains a pointer to each parent so that we can traverse the trie from the bottom up.\n",
    "\n",
    "Using this Trie, the FP-Growth can generate all paths that end in a specific item. This is done by first accessing all nodes that contain that item (by traversing the linked list). Then we go up to their parents and propagate the value of the node, and more than two paths contribute to parent, then we add the values. We continue going up until we reach the root of the tree, pruning nodes when their value becomes infrequent.\n",
    "\n",
    "Now the result is paths from the root to nodes with the with the suffix item of interest. For each path we collect all subsets and the keep the minimum value of that subset. If the same subset was encountered in more than one paths, we add the values. This way we now have all frequent subsets that end in that item with their frequency being the node's value.  \n",
    "\n",
    "### (c) (ii) Complexity of the FP-Growth Algorithm\n",
    "\n",
    "In the extreme case where the resulting Trie is a full tree, then the algorithm will have to solve an exponentially large number of subproblem and then merge their result. Therefore it is exponential with respect to the input. If the output of the FP-Growth Algorithm is all frequent itemsets, then again due to a possible \"bad\" tree structure, the complexity can be exponential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) A-Priori and FP-Growth Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singleton Frequencies: [('d', 9), ('a', 7), ('c', 7), ('b', 6)]\n",
      "\n",
      "Baskets:\n",
      "[['d', 'a', 'c'],\n",
      " ['d', 'a'],\n",
      " ['d', 'c', 'b'],\n",
      " ['d', 'b'],\n",
      " ['a', 'c'],\n",
      " ['d', 'a', 'c', 'b'],\n",
      " ['a', 'c'],\n",
      " ['d', 'a'],\n",
      " ['d', 'b'],\n",
      " ['d', 'a', 'b'],\n",
      " ['d', 'c'],\n",
      " ['c', 'b']]\n",
      "\n",
      "Pair Frequencies:\n",
      "[(('d', 'a'), 5),\n",
      " (('d', 'b'), 5),\n",
      " (('d', 'c'), 4),\n",
      " (('a', 'c'), 4),\n",
      " (('c', 'b'), 3),\n",
      " (('a', 'b'), 2)]\n"
     ]
    }
   ],
   "source": [
    "# Calculating the correct results \n",
    "baskets = [\n",
    "    ['a', 'c', 'd'],\n",
    "    ['a', 'd'],\n",
    "    ['b', 'c', 'd'],\n",
    "    ['b', 'd'],\n",
    "    ['a', 'c'],\n",
    "    ['a', 'b', 'c', 'd'],\n",
    "    ['a', 'c'],\n",
    "    ['a', 'd'],\n",
    "    ['b', 'd'],\n",
    "    ['a', 'b', 'd'],\n",
    "    ['c', 'd'],\n",
    "    ['b', 'c'],\n",
    "]\n",
    "\n",
    "freqs = collections.Counter(itertools.chain.from_iterable(baskets))\n",
    "for basket in baskets:\n",
    "    # Sort each basket by descending frequency for FP-Growth\n",
    "    basket.sort(key=freqs.get, reverse=True)\n",
    "    \n",
    "pair_freqs = collections.Counter()\n",
    "for basket in baskets:\n",
    "    pair_freqs.update(itertools.combinations(basket, 2))\n",
    "pair_freqs\n",
    "    \n",
    "print(f'Singleton Frequencies: {freqs.most_common()}')\n",
    "print()\n",
    "print('Baskets:') \n",
    "pprint(baskets)\n",
    "print()\n",
    "print('Pair Frequencies:')\n",
    "pprint(pair_freqs.most_common())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A-Priori Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see above, the every singleton is frequent.\n",
    "Therefore we construct all possible pairs and count their frequencies.\n",
    "We then observe that only `('d', 'a'), ('d', 'b'), ('d', 'c'), ('a', 'c')` are frequent.\n",
    "Next, using only letters from the above pairs we construct triples and observe that none of it is frequent, and thus the algorithm stops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FP-Growth Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/fp-growth_1.jpg' width=300>\n",
    "<br>\n",
    "<img src='images/fp-growth_2.jpg' width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e) Toivonen example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample:\n",
      "[['d', 'a', 'c'], ['d', 'a'], ['d', 'c', 'b']]\n",
      "\n",
      "Frequencies:\n",
      "[(('d',), 3), (('a',), 2), (('c',), 2), (('b',), 1)]\n",
      "[(('d', 'a'), 2),\n",
      " (('d', 'c'), 2),\n",
      " (('a', 'c'), 1),\n",
      " (('d', 'b'), 1),\n",
      " (('c', 'b'), 1)]\n",
      "[(('d', 'a', 'c'), 1), (('d', 'c', 'b'), 1)]\n",
      "\n",
      "All pairs and triples\n",
      "[('d', 'a'), ('d', 'c'), ('d', 'b'), ('a', 'c'), ('a', 'b'), ('c', 'b')]\n",
      "[('d', 'a', 'c'), ('d', 'a', 'b'), ('d', 'c', 'b'), ('a', 'c', 'b')]\n"
     ]
    }
   ],
   "source": [
    "def get_freqs(baskets, k):\n",
    "    freqs = collections.Counter()\n",
    "    for basket in baskets:\n",
    "        freqs.update(itertools.combinations(basket, k))\n",
    "    return freqs.most_common()\n",
    "\n",
    "\n",
    "sample = baskets[:3]\n",
    "print('Sample:')\n",
    "pprint(sample)\n",
    "print()\n",
    "print('Frequencies:')\n",
    "for k in range(1, 4):\n",
    "    pprint(get_freqs(sample, k))\n",
    "print()\n",
    "print('All pairs and triples')\n",
    "pprint(list(itertools.combinations('dacb', 2)))\n",
    "pprint(list(itertools.combinations('dacb', 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case: threshold = 1\n",
    "Border: $\\{A, B\\}$\n",
    "\n",
    "The sample border point has frequency 2 > threshold = 1, so the algorithm fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freqs\n",
      "{frozenset({'d'}): 9,\n",
      " frozenset({'a'}): 7,\n",
      " frozenset({'c'}): 7,\n",
      " frozenset({'b'}): 6,\n",
      " frozenset({'d', 'a'}): 5,\n",
      " frozenset({'d', 'c'}): 4,\n",
      " frozenset({'a', 'c'}): 4,\n",
      " frozenset({'d', 'b'}): 5,\n",
      " frozenset({'b', 'c'}): 3,\n",
      " frozenset({'d', 'a', 'c'}): 2,\n",
      " frozenset({'b', 'd', 'c'}): 2}\n",
      "\n",
      "Border Freqs:\n",
      "{frozenset({'a', 'b'}): 2}\n"
     ]
    }
   ],
   "source": [
    "def toivonen(threshold, border):\n",
    "    itemsets = {}\n",
    "    for k in range(1, 4):\n",
    "        for t, freq in get_freqs(sample, k):\n",
    "            if freq >= threshold:\n",
    "                itemsets[frozenset(t)] = 0\n",
    "\n",
    "    for basket in map(set, baskets):\n",
    "        for counter in itemsets, border:\n",
    "            for itemset in counter:\n",
    "                if itemset <= basket:\n",
    "                    counter[itemset] += 1\n",
    "\n",
    "    print('Freqs')\n",
    "    pprint(itemsets)\n",
    "    print()\n",
    "    print('Border Freqs:')\n",
    "    pprint(border)\n",
    "\n",
    "\n",
    "threshold = 1\n",
    "border = {frozenset(['a', 'b']): 0}\n",
    "toivonen(threshold, border)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case: threshold = 2\n",
    "\n",
    "Border: $\\{B\\}, \\{D, A, C\\}$\n",
    "\n",
    "The sample border point $\\{B\\}$ has frequency 6 > threshold = 2, so the algorithm fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freqs\n",
      "{frozenset({'d'}): 9,\n",
      " frozenset({'a'}): 7,\n",
      " frozenset({'c'}): 7,\n",
      " frozenset({'d', 'a'}): 5,\n",
      " frozenset({'d', 'c'}): 4}\n",
      "\n",
      "Border Freqs:\n",
      "{frozenset({'b'}): 6, frozenset({'d', 'a', 'c'}): 2}\n"
     ]
    }
   ],
   "source": [
    "threshold = 2\n",
    "border = {frozenset(['b']): 0, frozenset(['d', 'a', 'c']): 0}\n",
    "toivonen(threshold, border)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "<img src=\"images/exercise3.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm that is going to be tested is **A fast APRIORI implementation** (fimi01). I tried to run all other algorithms in the website using Ubuntu OS, but I was either receiving various compilation errors, or segmentation errors, or the algorithm was running indefinitely producing multiple gigabyte files even when the thresholds were set extremely high.\n",
    "\n",
    "The datasets on which the algorithm is going to be tested are **kosarak**, **accidents** and **pumsb**.\n",
    "\n",
    "I used a modified version of [this shell script](http://www.cs.bme.hu/~bodon/en/apriori/test_running_time) to measure the times needed by the algorithm to find all itemsets given thresholds 90%, 80%, 70%, and 60%. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "<img src=\"images/exercise4.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/5-2-2.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/figure-5-4.png\" width=\"300\">\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "    0   & 1/2 & 0 & 0   & 0 \\\\\n",
    "    1/3 & 0   & 0 & 1/2 & 0 \\\\\n",
    "    1/3 & 0   & 0 & 1/2 & 0 \\\\\n",
    "    1/3 & 1/2 & 0 & 0   & 0 \\\\\n",
    "    0   & 0   & 1 & 0   & 0 \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "<img src=\"images/figure-5-7.png\" width=\"300\">\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "1/3 & 1/2 & 0   \\\\\n",
    "1/3 & 0   & 1/2 \\\\\n",
    "1/3 & 1/2 & 1/2 \\\\\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/5-2-3.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/figure-5-3.png\" width=\"300\">\n",
    "\n",
    "* $M_{11}: \\{A, B\\} \\to \\{A, B\\}$\n",
    "\n",
    "    | Source | Out-Degree | Destinations\n",
    "    | --- | --- | --- |\n",
    "    | A | 3 | B |\n",
    "    | B | 2 | A |\n",
    "\n",
    "    <br>\n",
    "    \n",
    "* $M_{12}: \\{A, B\\} \\to \\{C, D\\}$\n",
    "\n",
    "    | Source | Out-Degree | Destinations\n",
    "    | --- | --- | --- |\n",
    "    | A | 3 | C, D |\n",
    "    | B | 2 | D |\n",
    "\n",
    "    <br>\n",
    "\n",
    "* $M_{21}: \\{C, D\\} \\to \\{A, B\\}$\n",
    "\n",
    "    | Source | Out-Degree | Destinations\n",
    "    | --- | --- | --- |\n",
    "    | C | 0 |  |\n",
    "    | D | 2 | B |\n",
    "\n",
    "    <br>\n",
    "\n",
    "* $M_{22}: \\{C, D\\} \\to \\{C, D\\}$\n",
    "\n",
    "    | Source | Out-Degree | Destinations\n",
    "    | --- | --- | --- |\n",
    "    | C | 0 |  |\n",
    "    | D | 2 | C |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/5-3-1.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/figure-5-15.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: [1 0 0 0]\n",
      "Pageranks: [0.42857143 0.19047619 0.19047619 0.19047619]\n",
      "\n",
      "Topic: [1 0 1 0]\n",
      "Pageranks: [0.38571429 0.17142857 0.27142857 0.17142857]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def topic_pagerank(transmat, topic, tax=0.2, n_iter=50):\n",
    "    v = np.ones(np.shape(transmat)[0])\n",
    "    v = v / np.sum(v)\n",
    "\n",
    "    taxed_transmat = (1 - tax) * transmat\n",
    "    teleport = (tax / np.sum(topic)) * topic\n",
    "    for _ in range(n_iter):\n",
    "        v = taxed_transmat @ v + teleport\n",
    "        \n",
    "    return v\n",
    "\n",
    "\n",
    "transmat = np.array([\n",
    "    [0, 1, 1, 0],\n",
    "    [1, 0, 0, 1],\n",
    "    [1, 0, 0, 1],\n",
    "    [1, 1, 0, 0],\n",
    "])\n",
    "transmat = transmat / np.sum(transmat, axis=0)\n",
    "\n",
    "topics = [\n",
    "    np.array([1,0,0,0]),\n",
    "    np.array([1,0,1,0]),\n",
    "]\n",
    "\n",
    "for topic in topics:\n",
    "    pageranks = topic_pagerank(transmat, topic)\n",
    "    print(f'Topic: {topic}\\nPageranks: {pageranks}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/5-4-2.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "761d1cff9000cfc09ac41353551509cfbc0507931ce4d3652274854167ea815b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('torchenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
